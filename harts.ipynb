{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9Aa3QRBqyR3qtEKSBBCSS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sooryakiran/HARTS/blob/master/harts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSowcQX6RfX9",
        "colab_type": "text"
      },
      "source": [
        "# Hardware Architectural Search (HARTS)\n",
        "--------------------------------------\n",
        "\n",
        "DISCLAIMER: This is not a binary neural network where the weights are zeros and ones.\n",
        "\n",
        "This notebook illustrates the use of ideas from deep learning to design hardwares for predictive applications. The below code can generate architectures to classify MNIST handwritten digits using just logic gates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zHHouR0PE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChtMmTbRSc_m",
        "colab_type": "text"
      },
      "source": [
        "# Choice of gates between any two values\n",
        "\n",
        "Here I have implimented 3 possible choices of gates, AND, OR and XOR. The choice has to be differentiable. So I used a weight value for each gate over which softmax will be taken to choose the gate. Since softmax is differentiable, the choice of gates become differentiable and we can use backpropagation to tune the weights to choose the gate that gives better results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ZnarMnVdDX",
        "colab_type": "text"
      },
      "source": [
        " 1 represents on state\n",
        " \n",
        "-1 represents off state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlxI9-nt0avh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Class BinLayer\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_neurons, temp = 0.01):\n",
        "        \"\"\"\n",
        "        The class constructor\n",
        "\n",
        "        @param in_neurons : int, bit size of input\n",
        "        @param temp       : float, temperature of softmax\n",
        "\n",
        "        Let us say that there are n bits as input. We have n x n possible input pairs.\n",
        "        However this reduces by nearly a factor of 2 for commutative bitwise operations.\n",
        "        In this module, we take the entire n x n as search domain, so that we can support\n",
        "        bitwise operations that are not commutative in future.\n",
        "\n",
        "        Assume we have an input bit vector x. We compute 3 output matrices,\n",
        "        OR  (x', x) \n",
        "        AND (x', x)\n",
        "        XOR (x', x)\n",
        "        Where x' is the transpose of x.\n",
        "\n",
        "        During forward pass, we choose one among theses 3 matrices element wise to\n",
        "        create an nxn matrix. The choice is done by doing a softmax over the stored weights.\n",
        "        The weight matrix is updated using backpropagation to improve the choice.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super(BinLayer, self).__init__()\n",
        "        self.in_neurons = in_neurons\n",
        "\n",
        "        self.and_ = AND()\n",
        "        self.or_  = OR()\n",
        "        self.xor_ = XOR()\n",
        "        \n",
        "        self.weights = torch.nn.Parameter(data = torch.Tensor(in_neurons*in_neurons, 3), requires_grad = True)\n",
        "        self.softmax = nn.Softmax(dim = -1)\n",
        "        self.temp    = temp\n",
        "\n",
        "        self.weights.data.uniform_(-1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward pass\n",
        "\n",
        "        @param x: input bit vector\n",
        "\n",
        "        \"\"\"\n",
        "        and_output = self.and_(x).unsqueeze(-1)\n",
        "        or_output  = self.or_(x).unsqueeze(-1)\n",
        "        xor_output = self.xor_(x).unsqueeze(-1)\n",
        "\n",
        "        soft_weights   = self.softmax(self.weights/self.temp)\n",
        "        massive_input  = torch.cat([and_output, or_output, xor_output], -1)\n",
        "        massive_weight = soft_weights.unsqueeze(0).repeat(x.size(0), 1, 1)\n",
        "        massive_output = torch.mul(massive_input, massive_weight)\n",
        "\n",
        "        return torch.sum(massive_output, axis = -1)\n",
        "\n",
        "class SELECT(nn.Module):\n",
        "    \"\"\"\n",
        "    The SELECT class\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_neurons, out_neurons, temp = 0.01):\n",
        "        \"\"\"\n",
        "        Since the BinLayer defined above is not scalable, i.e it produces nxn outputs\n",
        "        for an input of size n, we define a module that subsamples the input. The sub\n",
        "        sampling choices are also trainable using backpropagation, since we use \n",
        "        softmax over the stored weights to perform the choice.\n",
        "\n",
        "        @param in_neurons  : int, input bit vector length\n",
        "        @param out_neurons : int, subsampled output bit vector length\n",
        "\n",
        "        \"\"\"\n",
        "        super(SELECT, self).__init__()\n",
        "        self.weights = torch.nn.Parameter(data = torch.Tensor(in_neurons, out_neurons), requires_grad = True)\n",
        "        self.softmax = nn.Softmax(dim = 0)\n",
        "        self.temp    = temp\n",
        "\n",
        "        self.weights.data.uniform_(-1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Selects the most suitable top out_neurons\n",
        "\n",
        "        \"\"\"\n",
        "        soft_weights = self.softmax(self.weights/self.temp)\n",
        "        return x @ soft_weights\n",
        "\n",
        "class NotLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    The NOT layer\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_neurons, temp = 0.01):\n",
        "        \"\"\"\n",
        "        The NOT layer can be used as a possible 'activation function'. This layer \n",
        "        chooses whether to put a NOT gate or not on each elements in the input bit\n",
        "        vector. Like above, the choice is differentiable.\n",
        "\n",
        "        @param in_neurons : int, input bit vector length\n",
        "        @param temp       : float, softmax temperature\n",
        "\n",
        "        \"\"\"\n",
        "        super(NotLayer, self).__init__()\n",
        "        self.weights = torch.nn.Parameter(data = torch.Tensor(in_neurons, 2), requires_grad = True)\n",
        "        self.softmax = nn.softmax(dim = -1)\n",
        "        self.temp    = temp\n",
        "\n",
        "        self.weights.data.uniform_(-1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward pass\n",
        "\n",
        "        \"\"\"\n",
        "        x_in            = x.unsqueeze(0)\n",
        "        x_compliment_in = -x_in\n",
        "        soft_weights    = self.softmax(self.weights/self.temp)\n",
        "        massive_inputs  = torch.cat([x_in, x_compliment_in], -1)\n",
        "        massive_weights = soft_weights.unsqueeze(0).repeat(x.size(0), 1)\n",
        "        massive_output  = torch.mul(massive_input, massive_weight)\n",
        "\n",
        "        return torch.sum(massive_output, axis = -1)\n",
        "        \n",
        "class XOR(nn.Module):\n",
        "    \"\"\"\n",
        "    The XOR module\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(XOR, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x      = x.unsqueeze(-1)\n",
        "        output = -x @ torch.transpose(x, 1, 2)\n",
        "\n",
        "        return output.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class AND(nn.Module):\n",
        "    \"\"\"\n",
        "    The AND module\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(AND, self).__init__()\n",
        "        self.xor = XOR();\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = (x+1)/2 \n",
        "        return - self.xor(x)*2 -1\n",
        "\n",
        "class OR(nn.Module):\n",
        "    \"\"\"\n",
        "    The OR module\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(OR, self).__init__()\n",
        "        self.xor  =  XOR()\n",
        "        self.and_ = AND();\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.xor(x) + self.and_(x) + 1"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMIAOHEW1XY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.select_1 = SELECT(in_neurons = 784, out_neurons = 256)\n",
        "        self.bin_1    = BinLayer(in_neurons = 256)\n",
        "\n",
        "        self.select_2 = SELECT(in_neurons = 256*256, out_neurons = 256)\n",
        "        self.bin_2    = BinLayer(in_neurons = 256)\n",
        "\n",
        "        self.select_3 = SELECT(in_neurons = 256*256, out_neurons = 128)\n",
        "        self.bin_3    = BinLayer(in_neurons = 128)\n",
        "\n",
        "        self.select_4 = SELECT(in_neurons = 128*128, out_neurons= 64)\n",
        "        self.bin_4    = BinLayer(in_neurons = 64)\n",
        "\n",
        "        self.select_5 = SELECT(in_neurons = 64*64, out_neurons = 32)\n",
        "        self.bin_5    = BinLayer(in_neurons = 32)\n",
        "\n",
        "        self.select_6 = SELECT(in_neurons = 32*32, out_neurons = 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.select_1(x)\n",
        "        x = self.bin_1(x)\n",
        "        x = self.select_2(x)\n",
        "        x = self.bin_2(x)\n",
        "        x = self.select_3(x)\n",
        "        x = self.bin_3(x)\n",
        "        x = self.select_4(x)\n",
        "        x = self.bin_4(x)\n",
        "        x = self.select_5(x)\n",
        "        x = self.bin_5(x)\n",
        "        x = self.select_6(x)\n",
        "        return x"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5IGHE_KXydx",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjWguayR-3yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "LEARNING_RATES = [1e-2, 1e-3]\n",
        "MOMENTUM = 0.5\n",
        "BATCH_SIZE = 512\n",
        "LOG_INTERVAL = 5"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_aKeLP6L5e4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomTransform:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, x):\n",
        "        x = x*2 - 1\n",
        "        return x.view(-1)\n",
        "\n",
        "TRAIN_LOADER = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                                           transform=torchvision.transforms.Compose([\n",
        "                                           torchvision.transforms.ToTensor(),\n",
        "                                           CustomTransform()])),\n",
        "                                           batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "TEST_LOADER = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                                          transform=torchvision.transforms.Compose([\n",
        "                                          torchvision.transforms.ToTensor(),\n",
        "                                          CustomTransform()])),\n",
        "                                          batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9L3FQrtNzLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(network, epoch):\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        print(\"TRAINING ON GPU\")\n",
        "\n",
        "    network = network.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if epoch < len(LEARNING_RATES):\n",
        "        lr = LEARNING_RATES[epoch]\n",
        "    else:\n",
        "        lr = LEARNING_RATES[-1]\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "    for index, (data, target) in enumerate(TRAIN_LOADER):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if index%LOG_INTERVAL == 0:\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            acc = pred.eq(target.data.view_as(pred)).sum()*1.0/BATCH_SIZE\n",
        "        \n",
        "            print(\"EPOCH: %d: BATCH (%d), \\tLOSS = %0.3f, \\tACC:%0.2f\" %(epoch, index, loss.item(), acc.item()))\n",
        "\n",
        "def test(network):\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        print(\"TESTING ON GPU\")\n",
        "\n",
        "    network = network.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "\n",
        "    for index, (data, target) in enumerate(TEST_LOADER):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = network(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        if index%LOG_INTERVAL == 0:\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            acc = pred.eq(target.data.view_as(pred)).sum()*1.0/BATCH_SIZE\n",
        "        \n",
        "            print(\"BATCH (%d), \\tLOSS = %0.3f, \\tACC:%0.2f\" %(index, loss.item(), acc.item()))\n",
        "\n",
        "\n",
        "def train_multiple_epochs(network):\n",
        "    for epoch in range(EPOCHS):\n",
        "        train(network, epoch)\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDsQXPjKN0RJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "847b77d9-b762-443a-d62f-f9778440374d"
      },
      "source": [
        "network = Network()\n",
        "train_multiple_epochs(network)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING ON GPU\n",
            "EPOCH: 0: BATCH (0), \tLOSS = 2.303, \tACC:0.11\n",
            "EPOCH: 0: BATCH (5), \tLOSS = 2.123, \tACC:0.19\n",
            "EPOCH: 0: BATCH (10), \tLOSS = 2.046, \tACC:0.24\n",
            "EPOCH: 0: BATCH (15), \tLOSS = 1.909, \tACC:0.27\n",
            "EPOCH: 0: BATCH (20), \tLOSS = 1.782, \tACC:0.40\n",
            "EPOCH: 0: BATCH (25), \tLOSS = 1.711, \tACC:0.41\n",
            "EPOCH: 0: BATCH (30), \tLOSS = 1.695, \tACC:0.42\n",
            "EPOCH: 0: BATCH (35), \tLOSS = 1.680, \tACC:0.44\n",
            "EPOCH: 0: BATCH (40), \tLOSS = 1.625, \tACC:0.47\n",
            "EPOCH: 0: BATCH (45), \tLOSS = 1.625, \tACC:0.43\n",
            "EPOCH: 0: BATCH (50), \tLOSS = 1.645, \tACC:0.46\n",
            "EPOCH: 0: BATCH (55), \tLOSS = 1.612, \tACC:0.46\n",
            "EPOCH: 0: BATCH (60), \tLOSS = 1.637, \tACC:0.49\n",
            "EPOCH: 0: BATCH (65), \tLOSS = 1.626, \tACC:0.48\n",
            "EPOCH: 0: BATCH (70), \tLOSS = 1.636, \tACC:0.49\n",
            "EPOCH: 0: BATCH (75), \tLOSS = 1.661, \tACC:0.42\n",
            "EPOCH: 0: BATCH (80), \tLOSS = 1.603, \tACC:0.49\n",
            "EPOCH: 0: BATCH (85), \tLOSS = 1.606, \tACC:0.47\n",
            "EPOCH: 0: BATCH (90), \tLOSS = 1.620, \tACC:0.49\n",
            "EPOCH: 0: BATCH (95), \tLOSS = 1.627, \tACC:0.48\n",
            "EPOCH: 0: BATCH (100), \tLOSS = 1.600, \tACC:0.49\n",
            "EPOCH: 0: BATCH (105), \tLOSS = 1.649, \tACC:0.49\n",
            "EPOCH: 0: BATCH (110), \tLOSS = 1.633, \tACC:0.48\n",
            "EPOCH: 0: BATCH (115), \tLOSS = 1.612, \tACC:0.47\n",
            "TRAINING ON GPU\n",
            "EPOCH: 1: BATCH (0), \tLOSS = 1.595, \tACC:0.49\n",
            "EPOCH: 1: BATCH (5), \tLOSS = 1.585, \tACC:0.50\n",
            "EPOCH: 1: BATCH (10), \tLOSS = 1.646, \tACC:0.44\n",
            "EPOCH: 1: BATCH (15), \tLOSS = 1.597, \tACC:0.52\n",
            "EPOCH: 1: BATCH (20), \tLOSS = 1.602, \tACC:0.47\n",
            "EPOCH: 1: BATCH (25), \tLOSS = 1.602, \tACC:0.50\n",
            "EPOCH: 1: BATCH (30), \tLOSS = 1.619, \tACC:0.50\n",
            "EPOCH: 1: BATCH (35), \tLOSS = 1.622, \tACC:0.50\n",
            "EPOCH: 1: BATCH (40), \tLOSS = 1.612, \tACC:0.51\n",
            "EPOCH: 1: BATCH (45), \tLOSS = 1.631, \tACC:0.49\n",
            "EPOCH: 1: BATCH (50), \tLOSS = 1.576, \tACC:0.51\n",
            "EPOCH: 1: BATCH (55), \tLOSS = 1.603, \tACC:0.50\n",
            "EPOCH: 1: BATCH (60), \tLOSS = 1.549, \tACC:0.55\n",
            "EPOCH: 1: BATCH (65), \tLOSS = 1.579, \tACC:0.56\n",
            "EPOCH: 1: BATCH (70), \tLOSS = 1.588, \tACC:0.54\n",
            "EPOCH: 1: BATCH (75), \tLOSS = 1.608, \tACC:0.52\n",
            "EPOCH: 1: BATCH (80), \tLOSS = 1.598, \tACC:0.48\n",
            "EPOCH: 1: BATCH (85), \tLOSS = 1.568, \tACC:0.55\n",
            "EPOCH: 1: BATCH (90), \tLOSS = 1.555, \tACC:0.56\n",
            "EPOCH: 1: BATCH (95), \tLOSS = 1.528, \tACC:0.56\n",
            "EPOCH: 1: BATCH (100), \tLOSS = 1.598, \tACC:0.54\n",
            "EPOCH: 1: BATCH (105), \tLOSS = 1.552, \tACC:0.54\n",
            "EPOCH: 1: BATCH (110), \tLOSS = 1.599, \tACC:0.53\n",
            "EPOCH: 1: BATCH (115), \tLOSS = 1.526, \tACC:0.56\n",
            "TRAINING ON GPU\n",
            "EPOCH: 2: BATCH (0), \tLOSS = 1.599, \tACC:0.47\n",
            "EPOCH: 2: BATCH (5), \tLOSS = 1.564, \tACC:0.51\n",
            "EPOCH: 2: BATCH (10), \tLOSS = 1.577, \tACC:0.53\n",
            "EPOCH: 2: BATCH (15), \tLOSS = 1.575, \tACC:0.51\n",
            "EPOCH: 2: BATCH (20), \tLOSS = 1.557, \tACC:0.50\n",
            "EPOCH: 2: BATCH (25), \tLOSS = 1.552, \tACC:0.52\n",
            "EPOCH: 2: BATCH (30), \tLOSS = 1.555, \tACC:0.54\n",
            "EPOCH: 2: BATCH (35), \tLOSS = 1.544, \tACC:0.54\n",
            "EPOCH: 2: BATCH (40), \tLOSS = 1.591, \tACC:0.52\n",
            "EPOCH: 2: BATCH (45), \tLOSS = 1.534, \tACC:0.54\n",
            "EPOCH: 2: BATCH (50), \tLOSS = 1.532, \tACC:0.55\n",
            "EPOCH: 2: BATCH (55), \tLOSS = 1.569, \tACC:0.54\n",
            "EPOCH: 2: BATCH (60), \tLOSS = 1.574, \tACC:0.53\n",
            "EPOCH: 2: BATCH (65), \tLOSS = 1.519, \tACC:0.56\n",
            "EPOCH: 2: BATCH (70), \tLOSS = 1.513, \tACC:0.56\n",
            "EPOCH: 2: BATCH (75), \tLOSS = 1.559, \tACC:0.55\n",
            "EPOCH: 2: BATCH (80), \tLOSS = 1.536, \tACC:0.53\n",
            "EPOCH: 2: BATCH (85), \tLOSS = 1.597, \tACC:0.50\n",
            "EPOCH: 2: BATCH (90), \tLOSS = 1.562, \tACC:0.51\n",
            "EPOCH: 2: BATCH (95), \tLOSS = 1.533, \tACC:0.58\n",
            "EPOCH: 2: BATCH (100), \tLOSS = 1.520, \tACC:0.53\n",
            "EPOCH: 2: BATCH (105), \tLOSS = 1.510, \tACC:0.55\n",
            "EPOCH: 2: BATCH (110), \tLOSS = 1.504, \tACC:0.53\n",
            "EPOCH: 2: BATCH (115), \tLOSS = 1.518, \tACC:0.55\n",
            "TRAINING ON GPU\n",
            "EPOCH: 3: BATCH (0), \tLOSS = 1.507, \tACC:0.54\n",
            "EPOCH: 3: BATCH (5), \tLOSS = 1.548, \tACC:0.48\n",
            "EPOCH: 3: BATCH (10), \tLOSS = 1.479, \tACC:0.56\n",
            "EPOCH: 3: BATCH (15), \tLOSS = 1.508, \tACC:0.57\n",
            "EPOCH: 3: BATCH (20), \tLOSS = 1.550, \tACC:0.52\n",
            "EPOCH: 3: BATCH (25), \tLOSS = 1.495, \tACC:0.57\n",
            "EPOCH: 3: BATCH (30), \tLOSS = 1.520, \tACC:0.55\n",
            "EPOCH: 3: BATCH (35), \tLOSS = 1.556, \tACC:0.53\n",
            "EPOCH: 3: BATCH (40), \tLOSS = 1.515, \tACC:0.57\n",
            "EPOCH: 3: BATCH (45), \tLOSS = 1.525, \tACC:0.56\n",
            "EPOCH: 3: BATCH (50), \tLOSS = 1.506, \tACC:0.56\n",
            "EPOCH: 3: BATCH (55), \tLOSS = 1.522, \tACC:0.55\n",
            "EPOCH: 3: BATCH (60), \tLOSS = 1.479, \tACC:0.59\n",
            "EPOCH: 3: BATCH (65), \tLOSS = 1.509, \tACC:0.55\n",
            "EPOCH: 3: BATCH (70), \tLOSS = 1.553, \tACC:0.54\n",
            "EPOCH: 3: BATCH (75), \tLOSS = 1.481, \tACC:0.57\n",
            "EPOCH: 3: BATCH (80), \tLOSS = 1.532, \tACC:0.52\n",
            "EPOCH: 3: BATCH (85), \tLOSS = 1.503, \tACC:0.56\n",
            "EPOCH: 3: BATCH (90), \tLOSS = 1.547, \tACC:0.53\n",
            "EPOCH: 3: BATCH (95), \tLOSS = 1.483, \tACC:0.60\n",
            "EPOCH: 3: BATCH (100), \tLOSS = 1.496, \tACC:0.56\n",
            "EPOCH: 3: BATCH (105), \tLOSS = 1.496, \tACC:0.55\n",
            "EPOCH: 3: BATCH (110), \tLOSS = 1.467, \tACC:0.58\n",
            "EPOCH: 3: BATCH (115), \tLOSS = 1.511, \tACC:0.55\n",
            "TRAINING ON GPU\n",
            "EPOCH: 4: BATCH (0), \tLOSS = 1.494, \tACC:0.58\n",
            "EPOCH: 4: BATCH (5), \tLOSS = 1.526, \tACC:0.53\n",
            "EPOCH: 4: BATCH (10), \tLOSS = 1.522, \tACC:0.55\n",
            "EPOCH: 4: BATCH (15), \tLOSS = 1.520, \tACC:0.52\n",
            "EPOCH: 4: BATCH (20), \tLOSS = 1.511, \tACC:0.55\n",
            "EPOCH: 4: BATCH (25), \tLOSS = 1.497, \tACC:0.57\n",
            "EPOCH: 4: BATCH (30), \tLOSS = 1.522, \tACC:0.54\n",
            "EPOCH: 4: BATCH (35), \tLOSS = 1.510, \tACC:0.55\n",
            "EPOCH: 4: BATCH (40), \tLOSS = 1.551, \tACC:0.54\n",
            "EPOCH: 4: BATCH (45), \tLOSS = 1.551, \tACC:0.54\n",
            "EPOCH: 4: BATCH (50), \tLOSS = 1.479, \tACC:0.60\n",
            "EPOCH: 4: BATCH (55), \tLOSS = 1.502, \tACC:0.54\n",
            "EPOCH: 4: BATCH (60), \tLOSS = 1.523, \tACC:0.56\n",
            "EPOCH: 4: BATCH (65), \tLOSS = 1.530, \tACC:0.54\n",
            "EPOCH: 4: BATCH (70), \tLOSS = 1.524, \tACC:0.52\n",
            "EPOCH: 4: BATCH (75), \tLOSS = 1.533, \tACC:0.53\n",
            "EPOCH: 4: BATCH (80), \tLOSS = 1.519, \tACC:0.56\n",
            "EPOCH: 4: BATCH (85), \tLOSS = 1.550, \tACC:0.53\n",
            "EPOCH: 4: BATCH (90), \tLOSS = 1.465, \tACC:0.60\n",
            "EPOCH: 4: BATCH (95), \tLOSS = 1.514, \tACC:0.55\n",
            "EPOCH: 4: BATCH (100), \tLOSS = 1.486, \tACC:0.56\n",
            "EPOCH: 4: BATCH (105), \tLOSS = 1.546, \tACC:0.54\n",
            "EPOCH: 4: BATCH (110), \tLOSS = 1.490, \tACC:0.57\n",
            "EPOCH: 4: BATCH (115), \tLOSS = 1.490, \tACC:0.60\n",
            "TRAINING ON GPU\n",
            "EPOCH: 5: BATCH (0), \tLOSS = 1.509, \tACC:0.56\n",
            "EPOCH: 5: BATCH (5), \tLOSS = 1.508, \tACC:0.55\n",
            "EPOCH: 5: BATCH (10), \tLOSS = 1.501, \tACC:0.55\n",
            "EPOCH: 5: BATCH (15), \tLOSS = 1.531, \tACC:0.54\n",
            "EPOCH: 5: BATCH (20), \tLOSS = 1.485, \tACC:0.57\n",
            "EPOCH: 5: BATCH (25), \tLOSS = 1.502, \tACC:0.58\n",
            "EPOCH: 5: BATCH (30), \tLOSS = 1.501, \tACC:0.56\n",
            "EPOCH: 5: BATCH (35), \tLOSS = 1.469, \tACC:0.58\n",
            "EPOCH: 5: BATCH (40), \tLOSS = 1.479, \tACC:0.58\n",
            "EPOCH: 5: BATCH (45), \tLOSS = 1.460, \tACC:0.58\n",
            "EPOCH: 5: BATCH (50), \tLOSS = 1.523, \tACC:0.53\n",
            "EPOCH: 5: BATCH (55), \tLOSS = 1.539, \tACC:0.54\n",
            "EPOCH: 5: BATCH (60), \tLOSS = 1.504, \tACC:0.57\n",
            "EPOCH: 5: BATCH (65), \tLOSS = 1.503, \tACC:0.59\n",
            "EPOCH: 5: BATCH (70), \tLOSS = 1.489, \tACC:0.59\n",
            "EPOCH: 5: BATCH (75), \tLOSS = 1.514, \tACC:0.57\n",
            "EPOCH: 5: BATCH (80), \tLOSS = 1.538, \tACC:0.55\n",
            "EPOCH: 5: BATCH (85), \tLOSS = 1.496, \tACC:0.58\n",
            "EPOCH: 5: BATCH (90), \tLOSS = 1.492, \tACC:0.56\n",
            "EPOCH: 5: BATCH (95), \tLOSS = 1.465, \tACC:0.59\n",
            "EPOCH: 5: BATCH (100), \tLOSS = 1.518, \tACC:0.58\n",
            "EPOCH: 5: BATCH (105), \tLOSS = 1.500, \tACC:0.56\n",
            "EPOCH: 5: BATCH (110), \tLOSS = 1.497, \tACC:0.56\n",
            "EPOCH: 5: BATCH (115), \tLOSS = 1.525, \tACC:0.56\n",
            "TRAINING ON GPU\n",
            "EPOCH: 6: BATCH (0), \tLOSS = 1.499, \tACC:0.56\n",
            "EPOCH: 6: BATCH (5), \tLOSS = 1.479, \tACC:0.61\n",
            "EPOCH: 6: BATCH (10), \tLOSS = 1.528, \tACC:0.57\n",
            "EPOCH: 6: BATCH (15), \tLOSS = 1.517, \tACC:0.56\n",
            "EPOCH: 6: BATCH (20), \tLOSS = 1.471, \tACC:0.59\n",
            "EPOCH: 6: BATCH (25), \tLOSS = 1.487, \tACC:0.55\n",
            "EPOCH: 6: BATCH (30), \tLOSS = 1.514, \tACC:0.55\n",
            "EPOCH: 6: BATCH (35), \tLOSS = 1.497, \tACC:0.58\n",
            "EPOCH: 6: BATCH (40), \tLOSS = 1.503, \tACC:0.54\n",
            "EPOCH: 6: BATCH (45), \tLOSS = 1.492, \tACC:0.60\n",
            "EPOCH: 6: BATCH (50), \tLOSS = 1.483, \tACC:0.61\n",
            "EPOCH: 6: BATCH (55), \tLOSS = 1.495, \tACC:0.58\n",
            "EPOCH: 6: BATCH (60), \tLOSS = 1.508, \tACC:0.59\n",
            "EPOCH: 6: BATCH (65), \tLOSS = 1.507, \tACC:0.56\n",
            "EPOCH: 6: BATCH (70), \tLOSS = 1.515, \tACC:0.59\n",
            "EPOCH: 6: BATCH (75), \tLOSS = 1.489, \tACC:0.58\n",
            "EPOCH: 6: BATCH (80), \tLOSS = 1.483, \tACC:0.58\n",
            "EPOCH: 6: BATCH (85), \tLOSS = 1.501, \tACC:0.59\n",
            "EPOCH: 6: BATCH (90), \tLOSS = 1.510, \tACC:0.58\n",
            "EPOCH: 6: BATCH (95), \tLOSS = 1.472, \tACC:0.59\n",
            "EPOCH: 6: BATCH (100), \tLOSS = 1.533, \tACC:0.54\n",
            "EPOCH: 6: BATCH (105), \tLOSS = 1.502, \tACC:0.59\n",
            "EPOCH: 6: BATCH (110), \tLOSS = 1.481, \tACC:0.57\n",
            "EPOCH: 6: BATCH (115), \tLOSS = 1.472, \tACC:0.63\n",
            "TRAINING ON GPU\n",
            "EPOCH: 7: BATCH (0), \tLOSS = 1.503, \tACC:0.55\n",
            "EPOCH: 7: BATCH (5), \tLOSS = 1.521, \tACC:0.56\n",
            "EPOCH: 7: BATCH (10), \tLOSS = 1.493, \tACC:0.59\n",
            "EPOCH: 7: BATCH (15), \tLOSS = 1.455, \tACC:0.60\n",
            "EPOCH: 7: BATCH (20), \tLOSS = 1.518, \tACC:0.56\n",
            "EPOCH: 7: BATCH (25), \tLOSS = 1.477, \tACC:0.57\n",
            "EPOCH: 7: BATCH (30), \tLOSS = 1.490, \tACC:0.57\n",
            "EPOCH: 7: BATCH (35), \tLOSS = 1.544, \tACC:0.58\n",
            "EPOCH: 7: BATCH (40), \tLOSS = 1.482, \tACC:0.56\n",
            "EPOCH: 7: BATCH (45), \tLOSS = 1.502, \tACC:0.57\n",
            "EPOCH: 7: BATCH (50), \tLOSS = 1.471, \tACC:0.59\n",
            "EPOCH: 7: BATCH (55), \tLOSS = 1.499, \tACC:0.59\n",
            "EPOCH: 7: BATCH (60), \tLOSS = 1.470, \tACC:0.61\n",
            "EPOCH: 7: BATCH (65), \tLOSS = 1.494, \tACC:0.58\n",
            "EPOCH: 7: BATCH (70), \tLOSS = 1.502, \tACC:0.57\n",
            "EPOCH: 7: BATCH (75), \tLOSS = 1.480, \tACC:0.58\n",
            "EPOCH: 7: BATCH (80), \tLOSS = 1.491, \tACC:0.56\n",
            "EPOCH: 7: BATCH (85), \tLOSS = 1.511, \tACC:0.55\n",
            "EPOCH: 7: BATCH (90), \tLOSS = 1.489, \tACC:0.58\n",
            "EPOCH: 7: BATCH (95), \tLOSS = 1.509, \tACC:0.58\n",
            "EPOCH: 7: BATCH (100), \tLOSS = 1.452, \tACC:0.60\n",
            "EPOCH: 7: BATCH (105), \tLOSS = 1.506, \tACC:0.59\n",
            "EPOCH: 7: BATCH (110), \tLOSS = 1.527, \tACC:0.55\n",
            "EPOCH: 7: BATCH (115), \tLOSS = 1.490, \tACC:0.57\n",
            "TRAINING ON GPU\n",
            "EPOCH: 8: BATCH (0), \tLOSS = 1.492, \tACC:0.62\n",
            "EPOCH: 8: BATCH (5), \tLOSS = 1.453, \tACC:0.60\n",
            "EPOCH: 8: BATCH (10), \tLOSS = 1.443, \tACC:0.57\n",
            "EPOCH: 8: BATCH (15), \tLOSS = 1.495, \tACC:0.58\n",
            "EPOCH: 8: BATCH (20), \tLOSS = 1.477, \tACC:0.60\n",
            "EPOCH: 8: BATCH (25), \tLOSS = 1.486, \tACC:0.59\n",
            "EPOCH: 8: BATCH (30), \tLOSS = 1.465, \tACC:0.56\n",
            "EPOCH: 8: BATCH (35), \tLOSS = 1.463, \tACC:0.58\n",
            "EPOCH: 8: BATCH (40), \tLOSS = 1.494, \tACC:0.57\n",
            "EPOCH: 8: BATCH (45), \tLOSS = 1.446, \tACC:0.60\n",
            "EPOCH: 8: BATCH (50), \tLOSS = 1.450, \tACC:0.59\n",
            "EPOCH: 8: BATCH (55), \tLOSS = 1.483, \tACC:0.60\n",
            "EPOCH: 8: BATCH (60), \tLOSS = 1.496, \tACC:0.54\n",
            "EPOCH: 8: BATCH (65), \tLOSS = 1.470, \tACC:0.58\n",
            "EPOCH: 8: BATCH (70), \tLOSS = 1.464, \tACC:0.57\n",
            "EPOCH: 8: BATCH (75), \tLOSS = 1.486, \tACC:0.61\n",
            "EPOCH: 8: BATCH (80), \tLOSS = 1.450, \tACC:0.61\n",
            "EPOCH: 8: BATCH (85), \tLOSS = 1.464, \tACC:0.59\n",
            "EPOCH: 8: BATCH (90), \tLOSS = 1.472, \tACC:0.55\n",
            "EPOCH: 8: BATCH (95), \tLOSS = 1.510, \tACC:0.56\n",
            "EPOCH: 8: BATCH (100), \tLOSS = 1.480, \tACC:0.57\n",
            "EPOCH: 8: BATCH (105), \tLOSS = 1.520, \tACC:0.59\n",
            "EPOCH: 8: BATCH (110), \tLOSS = 1.467, \tACC:0.61\n",
            "EPOCH: 8: BATCH (115), \tLOSS = 1.433, \tACC:0.60\n",
            "TRAINING ON GPU\n",
            "EPOCH: 9: BATCH (0), \tLOSS = 1.449, \tACC:0.58\n",
            "EPOCH: 9: BATCH (5), \tLOSS = 1.452, \tACC:0.59\n",
            "EPOCH: 9: BATCH (10), \tLOSS = 1.452, \tACC:0.62\n",
            "EPOCH: 9: BATCH (15), \tLOSS = 1.508, \tACC:0.58\n",
            "EPOCH: 9: BATCH (20), \tLOSS = 1.448, \tACC:0.60\n",
            "EPOCH: 9: BATCH (25), \tLOSS = 1.448, \tACC:0.60\n",
            "EPOCH: 9: BATCH (30), \tLOSS = 1.470, \tACC:0.59\n",
            "EPOCH: 9: BATCH (35), \tLOSS = 1.443, \tACC:0.61\n",
            "EPOCH: 9: BATCH (40), \tLOSS = 1.494, \tACC:0.59\n",
            "EPOCH: 9: BATCH (45), \tLOSS = 1.464, \tACC:0.62\n",
            "EPOCH: 9: BATCH (50), \tLOSS = 1.441, \tACC:0.59\n",
            "EPOCH: 9: BATCH (55), \tLOSS = 1.477, \tACC:0.58\n",
            "EPOCH: 9: BATCH (60), \tLOSS = 1.462, \tACC:0.59\n",
            "EPOCH: 9: BATCH (65), \tLOSS = 1.497, \tACC:0.58\n",
            "EPOCH: 9: BATCH (70), \tLOSS = 1.490, \tACC:0.57\n",
            "EPOCH: 9: BATCH (75), \tLOSS = 1.476, \tACC:0.57\n",
            "EPOCH: 9: BATCH (80), \tLOSS = 1.493, \tACC:0.55\n",
            "EPOCH: 9: BATCH (85), \tLOSS = 1.463, \tACC:0.60\n",
            "EPOCH: 9: BATCH (90), \tLOSS = 1.468, \tACC:0.59\n",
            "EPOCH: 9: BATCH (95), \tLOSS = 1.454, \tACC:0.60\n",
            "EPOCH: 9: BATCH (100), \tLOSS = 1.453, \tACC:0.61\n",
            "EPOCH: 9: BATCH (105), \tLOSS = 1.464, \tACC:0.59\n",
            "EPOCH: 9: BATCH (110), \tLOSS = 1.471, \tACC:0.59\n",
            "EPOCH: 9: BATCH (115), \tLOSS = 1.498, \tACC:0.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTZ-0_aJQTCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "114c4186-5fdd-4e6c-d6a2-0c891fb26122"
      },
      "source": [
        "torch.save(network, \"saved_model.pth\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SELECT. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BinLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type AND. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type XOR. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type OR. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIL0TEMKb4z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "fccb87e5-d66d-4d98-d947-21ed4a912ed5"
      },
      "source": [
        "test(network)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TESTING ON GPU\n",
            "BATCH (0), \tLOSS = 1.428, \tACC:0.60\n",
            "BATCH (5), \tLOSS = 1.479, \tACC:0.56\n",
            "BATCH (10), \tLOSS = 1.495, \tACC:0.54\n",
            "BATCH (15), \tLOSS = 1.469, \tACC:0.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFJM-8UmX7Y0",
        "colab_type": "text"
      },
      "source": [
        "# Check the confidence of each choice so that we can freeze and convert them into gates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e99A9C-HO2Oc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "3a8c7126-9ab3-477e-9197-76fa8d76a732"
      },
      "source": [
        "for name, param in network.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        if \"select\" in name:\n",
        "            print(\"SELECT\")\n",
        "            softmax = nn.Softmax(dim = 0)\n",
        "            weights = param.data / 0.01\n",
        "            outs = torch.transpose(softmax(weights), 0, 1)\n",
        "            outs, _ = torch.max(outs, dim = -1)\n",
        "            print(torch.mean(outs))\n",
        "            \n",
        "        elif \"bin\" in name:\n",
        "            print(\"BIN\")\n",
        "            softmax = nn.Softmax(dim = -1)\n",
        "            weights = param.data / 0.01\n",
        "            outs, _ = torch.max(softmax(weights), dim = -1)\n",
        "            print(torch.mean(outs))\n",
        "        else:\n",
        "            print(\"UNKNOWN\")\n",
        "        #print(name, param.data)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SELECT\n",
            "tensor(0.9628, device='cuda:0')\n",
            "BIN\n",
            "tensor(0.9976, device='cuda:0')\n",
            "SELECT\n",
            "tensor(0.9663, device='cuda:0')\n",
            "BIN\n",
            "tensor(0.9983, device='cuda:0')\n",
            "SELECT\n",
            "tensor(0.9578, device='cuda:0')\n",
            "BIN\n",
            "tensor(0.9974, device='cuda:0')\n",
            "SELECT\n",
            "tensor(0.9428, device='cuda:0')\n",
            "BIN\n",
            "tensor(0.9969, device='cuda:0')\n",
            "SELECT\n",
            "tensor(0.9087, device='cuda:0')\n",
            "BIN\n",
            "tensor(0.9949, device='cuda:0')\n",
            "SELECT\n",
            "tensor(1., device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp9mm14VO9qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    }
  ]
}