{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPC/37VOoZfyiyWvfTs034",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sooryakiran/HARTS/blob/master/Harts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSowcQX6RfX9",
        "colab_type": "text"
      },
      "source": [
        "# Hardware Architectural Search (HARTS)\n",
        "--------------------------------------\n",
        "\n",
        "DISCLAIMER: This is not a binary neural network where the weights are zeros and ones.\n",
        "\n",
        "This notebook illustrates the use of ideas from deep learning to design hardwares for predictive applications. The below code can generate architectures to classify MNIST handwritten digits using just logic gates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zHHouR0PE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChtMmTbRSc_m",
        "colab_type": "text"
      },
      "source": [
        "# Choice of gates between any two values\n",
        "\n",
        "Here I have implimented 3 possible choices of gates, AND, OR and XOR. The choice has to be differentiable. So I used a weight value for each gate over which softmax will be taken to choose the gate. Since softmax is differentiable, the choice of gates become differentiable and we can use backpropagation to tune the weights to choose the gate that gives better results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8ZnarMnVdDX",
        "colab_type": "text"
      },
      "source": [
        " 1 represents on state\n",
        " \n",
        "-1 represents off state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlxI9-nt0avh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Class BinLayer\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_neurons, temp = 0.01):\n",
        "        \"\"\"\n",
        "        The class constructor\n",
        "\n",
        "        @param in_neurons : int, bit size of input\n",
        "        @param temp       : float, temperature of softmax\n",
        "\n",
        "        Let us say that there are n bits as input. We have n x n possible input pairs.\n",
        "        However this reduces by nearly a factor of 2 for commutative bitwise operations.\n",
        "        In this module, we take the entire n x n as search domain, so that we can support\n",
        "        bitwise operations that are not commutative in future.\n",
        "\n",
        "        Assume we have an input bit vector x. We compute 3 output matrices,\n",
        "        OR  (x', x) \n",
        "        AND (x', x)\n",
        "        XOR (x', x)\n",
        "        Where x' is the transpose of x.\n",
        "\n",
        "        During forward pass, we choose one among theses 3 matrices element wise to\n",
        "        create an nxn matrix. The choice is done by doing a softmax over the stored weights.\n",
        "        The weight matrix is updated using backpropagation to improve the choice.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super(BinLayer, self).__init__()\n",
        "        self.in_neurons = in_neurons\n",
        "\n",
        "        self.and_ = AND()\n",
        "        self.or_  = OR()\n",
        "        self.xor_ = XOR()\n",
        "        \n",
        "        self.weights = torch.nn.Parameter(data = torch.Tensor(in_neurons*in_neurons, 3), requires_grad = True)\n",
        "        self.softmax = nn.Softmax(dim = -1)\n",
        "        self.temp    = temp\n",
        "\n",
        "        self.weights.data.uniform_(-1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward pass\n",
        "\n",
        "        @param x: input bit vector\n",
        "\n",
        "        \"\"\"\n",
        "        and_output = self.and_(x).unsqueeze(-1)\n",
        "        or_output  = self.or_(x).unsqueeze(-1)\n",
        "        xor_output = self.xor_(x).unsqueeze(-1)\n",
        "\n",
        "        soft_weights   = self.softmax(self.weights/self.temp)\n",
        "        massive_input  = torch.cat([and_output, or_output, xor_output], -1)\n",
        "        massive_weight = soft_weights.unsqueeze(0).repeat(x.size(0), 1, 1)\n",
        "        massive_output = torch.mul(massive_input, massive_weight)\n",
        "\n",
        "        return torch.sum(massive_output, axis = -1)\n",
        "\n",
        "class SELECT(nn.Module):\n",
        "    \"\"\"\n",
        "    The SELECT class\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_neurons, out_neurons, temp = 0.01):\n",
        "        \"\"\"\n",
        "        Since the BinLayer defined above is not scalable, i.e it produces nxn outputs\n",
        "        for an input of size n, we define a module that subsamples the input. The sub\n",
        "        sampling choices are also trainable using backpropagation, since we use \n",
        "        softmax over the stored weights to perform the choice.\n",
        "\n",
        "        @param in_neurons  : int, input bit vector length\n",
        "        @param out_neurons : int, subsampled output bit vector length\n",
        "\n",
        "        \"\"\"\n",
        "        super(SELECT, self).__init__()\n",
        "        self.weights = torch.nn.Parameter(data = torch.Tensor(in_neurons, out_neurons), requires_grad = True)\n",
        "        self.softmax = nn.Softmax(dim = 0)\n",
        "        self.temp    = temp\n",
        "\n",
        "        self.weights.data.uniform_(-1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Selects the most suitable top out_neurons\n",
        "\n",
        "        \"\"\"\n",
        "        soft_weights = self.softmax(self.weights/self.temp)\n",
        "        return x @ soft_weights\n",
        "\n",
        "class NotLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    The NOT layer\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_neurons, temp = 0.01):\n",
        "        \"\"\"\n",
        "        The NOT layer can be used as a possible 'activation function'. This layer \n",
        "        chooses whether to put a NOT gate or not on each elements in the input bit\n",
        "        vector. Like above, the choice is differentiable.\n",
        "\n",
        "        @param in_neurons : int, input bit vector length\n",
        "        @param temp       : float, softmax temperature\n",
        "\n",
        "        \"\"\"\n",
        "        super(NotLayer, self).__init__()\n",
        "        self.weights = torch.nn.Parameter(data = torch.Tensor(in_neurons), requires_grad = True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.temp    = temp\n",
        "\n",
        "        self.weights.data.uniform_(-1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward pass\n",
        "\n",
        "        \"\"\"\n",
        "        x_in            = x\n",
        "        x_compliment_in = -x_in\n",
        "        soft_weights    = self.sigmoid(self.weights/self.temp)\n",
        "        return torch.mul(x_in, soft_weights) + torch.mul(x_compliment_in, 1 - selft_weights)\n",
        "\n",
        "class XOR(nn.Module):\n",
        "    \"\"\"\n",
        "    The XOR module\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(XOR, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x      = x.unsqueeze(-1)\n",
        "        output = -x @ torch.transpose(x, 1, 2)\n",
        "\n",
        "        return output.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class AND(nn.Module):\n",
        "    \"\"\"\n",
        "    The AND module\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(AND, self).__init__()\n",
        "        self.xor = XOR();\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = (x+1)/2 \n",
        "        return - self.xor(x)*2 -1\n",
        "\n",
        "class OR(nn.Module):\n",
        "    \"\"\"\n",
        "    The OR module\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(OR, self).__init__()\n",
        "        self.xor  =  XOR()\n",
        "        self.and_ = AND();\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.xor(x) + self.and_(x) + 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMIAOHEW1XY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "\n",
        "        self.select_1 = SELECT(in_neurons = 784, out_neurons = 256)\n",
        "        self.bin_1    = BinLayer(in_neurons = 256)\n",
        "\n",
        "        self.select_2 = SELECT(in_neurons = 256*256, out_neurons = 256)\n",
        "        self.not_2    = NotLayer(in_neurons = 256)\n",
        "        self.bin_2    = BinLayer(in_neurons = 256)\n",
        "\n",
        "        self.select_3 = SELECT(in_neurons = 256*256, out_neurons = 128)\n",
        "        self.not_3    = NotLayer(in_neurons = 128)\n",
        "        self.bin_3    = BinLayer(in_neurons = 128)\n",
        "\n",
        "        self.select_4 = SELECT(in_neurons = 128*128, out_neurons= 64)\n",
        "        self.not_4    = NotLayer(in_neurons = 64)\n",
        "        self.bin_4    = BinLayer(in_neurons = 64)\n",
        "\n",
        "        self.select_5 = SELECT(in_neurons = 64*64, out_neurons = 32)\n",
        "        self.not_5    = NotLayer(in_neurons = 32)\n",
        "        self.bin_5    = BinLayer(in_neurons = 32)\n",
        "\n",
        "        self.select_6 = SELECT(in_neurons = 32*32, out_neurons = 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.select_1(x)\n",
        "        x = self.bin_1(x)\n",
        "\n",
        "        x = self.select_2(x)\n",
        "        #x = self.not_2(x)\n",
        "        x = self.bin_2(x)\n",
        "\n",
        "        x = self.select_3(x)\n",
        "        #x = self.not_3(x)\n",
        "        x = self.bin_3(x)\n",
        "\n",
        "        x = self.select_4(x)\n",
        "        #x = self.not_4(x)\n",
        "        x = self.bin_4(x)\n",
        "\n",
        "        x = self.select_5(x)\n",
        "        #x = self.not_5(x)\n",
        "        x = self.bin_5(x)\n",
        "        x = self.select_6(x)\n",
        "        return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5IGHE_KXydx",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjWguayR-3yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "LEARNING_RATES = [1e-2, 1e-3]\n",
        "MOMENTUM = 0.5\n",
        "BATCH_SIZE = 512\n",
        "LOG_INTERVAL = 5"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_aKeLP6L5e4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomTransform:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def __call__(self, x):\n",
        "        x = x*2 - 1\n",
        "        return x.view(-1)\n",
        "\n",
        "TRAIN_LOADER = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                                           transform=torchvision.transforms.Compose([\n",
        "                                           torchvision.transforms.ToTensor(),\n",
        "                                           CustomTransform()])),\n",
        "                                           batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "TEST_LOADER = torch.utils.data.DataLoader(torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                                          transform=torchvision.transforms.Compose([\n",
        "                                          torchvision.transforms.ToTensor(),\n",
        "                                          CustomTransform()])),\n",
        "                                          batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9L3FQrtNzLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(network, epoch):\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        print(\"TRAINING ON GPU\")\n",
        "\n",
        "    network = network.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if epoch < len(LEARNING_RATES):\n",
        "        lr = LEARNING_RATES[epoch]\n",
        "    else:\n",
        "        lr = LEARNING_RATES[-1]\n",
        "    optimizer = torch.optim.Adam(network.parameters(), lr=lr)\n",
        "    for index, (data, target) in enumerate(TRAIN_LOADER):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = network(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if index%LOG_INTERVAL == 0:\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            acc = pred.eq(target.data.view_as(pred)).sum()*1.0/BATCH_SIZE\n",
        "        \n",
        "            print(\"EPOCH: %d: BATCH (%d), \\tLOSS = %0.3f, \\tACC:%0.2f\" %(epoch, index, loss.item(), acc.item()))\n",
        "\n",
        "def test(network):\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        print(\"TESTING ON GPU\")\n",
        "\n",
        "    network = network.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "\n",
        "    for index, (data, target) in enumerate(TEST_LOADER):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = network(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        if index%LOG_INTERVAL == 0:\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            acc = pred.eq(target.data.view_as(pred)).sum()*1.0/BATCH_SIZE\n",
        "        \n",
        "            print(\"BATCH (%d), \\tLOSS = %0.3f, \\tACC:%0.2f\" %(index, loss.item(), acc.item()))\n",
        "\n",
        "\n",
        "def train_multiple_epochs(network):\n",
        "    for epoch in range(EPOCHS):\n",
        "        train(network, epoch)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDsQXPjKN0RJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57ee5dc9-8434-42f9-f4c3-7c7f047346dc"
      },
      "source": [
        "network = Network()\n",
        "train_multiple_epochs(network)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING ON GPU\n",
            "EPOCH: 0: BATCH (0), \tLOSS = 2.306, \tACC:0.11\n",
            "EPOCH: 0: BATCH (5), \tLOSS = 2.155, \tACC:0.17\n",
            "EPOCH: 0: BATCH (10), \tLOSS = 2.061, \tACC:0.26\n",
            "EPOCH: 0: BATCH (15), \tLOSS = 1.892, \tACC:0.36\n",
            "EPOCH: 0: BATCH (20), \tLOSS = 1.668, \tACC:0.55\n",
            "EPOCH: 0: BATCH (25), \tLOSS = 1.549, \tACC:0.59\n",
            "EPOCH: 0: BATCH (30), \tLOSS = 1.601, \tACC:0.54\n",
            "EPOCH: 0: BATCH (35), \tLOSS = 1.612, \tACC:0.53\n",
            "EPOCH: 0: BATCH (40), \tLOSS = 1.550, \tACC:0.55\n",
            "EPOCH: 0: BATCH (45), \tLOSS = 1.538, \tACC:0.62\n",
            "EPOCH: 0: BATCH (50), \tLOSS = 1.507, \tACC:0.61\n",
            "EPOCH: 0: BATCH (55), \tLOSS = 1.529, \tACC:0.56\n",
            "EPOCH: 0: BATCH (60), \tLOSS = 1.513, \tACC:0.59\n",
            "EPOCH: 0: BATCH (65), \tLOSS = 1.560, \tACC:0.57\n",
            "EPOCH: 0: BATCH (70), \tLOSS = 1.523, \tACC:0.61\n",
            "EPOCH: 0: BATCH (75), \tLOSS = 1.553, \tACC:0.60\n",
            "EPOCH: 0: BATCH (80), \tLOSS = 1.563, \tACC:0.60\n",
            "EPOCH: 0: BATCH (85), \tLOSS = 1.520, \tACC:0.59\n",
            "EPOCH: 0: BATCH (90), \tLOSS = 1.507, \tACC:0.61\n",
            "EPOCH: 0: BATCH (95), \tLOSS = 1.558, \tACC:0.57\n",
            "EPOCH: 0: BATCH (100), \tLOSS = 1.565, \tACC:0.56\n",
            "EPOCH: 0: BATCH (105), \tLOSS = 1.541, \tACC:0.59\n",
            "EPOCH: 0: BATCH (110), \tLOSS = 1.503, \tACC:0.61\n",
            "EPOCH: 0: BATCH (115), \tLOSS = 1.526, \tACC:0.57\n",
            "TRAINING ON GPU\n",
            "EPOCH: 1: BATCH (0), \tLOSS = 1.528, \tACC:0.61\n",
            "EPOCH: 1: BATCH (5), \tLOSS = 1.505, \tACC:0.62\n",
            "EPOCH: 1: BATCH (10), \tLOSS = 1.451, \tACC:0.62\n",
            "EPOCH: 1: BATCH (15), \tLOSS = 1.507, \tACC:0.60\n",
            "EPOCH: 1: BATCH (20), \tLOSS = 1.494, \tACC:0.61\n",
            "EPOCH: 1: BATCH (25), \tLOSS = 1.530, \tACC:0.56\n",
            "EPOCH: 1: BATCH (30), \tLOSS = 1.520, \tACC:0.58\n",
            "EPOCH: 1: BATCH (35), \tLOSS = 1.493, \tACC:0.61\n",
            "EPOCH: 1: BATCH (40), \tLOSS = 1.495, \tACC:0.60\n",
            "EPOCH: 1: BATCH (45), \tLOSS = 1.507, \tACC:0.59\n",
            "EPOCH: 1: BATCH (50), \tLOSS = 1.518, \tACC:0.61\n",
            "EPOCH: 1: BATCH (55), \tLOSS = 1.515, \tACC:0.62\n",
            "EPOCH: 1: BATCH (60), \tLOSS = 1.528, \tACC:0.58\n",
            "EPOCH: 1: BATCH (65), \tLOSS = 1.490, \tACC:0.64\n",
            "EPOCH: 1: BATCH (70), \tLOSS = 1.510, \tACC:0.60\n",
            "EPOCH: 1: BATCH (75), \tLOSS = 1.433, \tACC:0.68\n",
            "EPOCH: 1: BATCH (80), \tLOSS = 1.495, \tACC:0.63\n",
            "EPOCH: 1: BATCH (85), \tLOSS = 1.498, \tACC:0.64\n",
            "EPOCH: 1: BATCH (90), \tLOSS = 1.470, \tACC:0.65\n",
            "EPOCH: 1: BATCH (95), \tLOSS = 1.507, \tACC:0.61\n",
            "EPOCH: 1: BATCH (100), \tLOSS = 1.525, \tACC:0.61\n",
            "EPOCH: 1: BATCH (105), \tLOSS = 1.471, \tACC:0.65\n",
            "EPOCH: 1: BATCH (110), \tLOSS = 1.520, \tACC:0.62\n",
            "EPOCH: 1: BATCH (115), \tLOSS = 1.501, \tACC:0.61\n",
            "TRAINING ON GPU\n",
            "EPOCH: 2: BATCH (0), \tLOSS = 1.503, \tACC:0.63\n",
            "EPOCH: 2: BATCH (5), \tLOSS = 1.497, \tACC:0.62\n",
            "EPOCH: 2: BATCH (10), \tLOSS = 1.447, \tACC:0.68\n",
            "EPOCH: 2: BATCH (15), \tLOSS = 1.459, \tACC:0.63\n",
            "EPOCH: 2: BATCH (20), \tLOSS = 1.484, \tACC:0.63\n",
            "EPOCH: 2: BATCH (25), \tLOSS = 1.517, \tACC:0.62\n",
            "EPOCH: 2: BATCH (30), \tLOSS = 1.492, \tACC:0.62\n",
            "EPOCH: 2: BATCH (35), \tLOSS = 1.469, \tACC:0.65\n",
            "EPOCH: 2: BATCH (40), \tLOSS = 1.514, \tACC:0.60\n",
            "EPOCH: 2: BATCH (45), \tLOSS = 1.461, \tACC:0.63\n",
            "EPOCH: 2: BATCH (50), \tLOSS = 1.486, \tACC:0.65\n",
            "EPOCH: 2: BATCH (55), \tLOSS = 1.463, \tACC:0.65\n",
            "EPOCH: 2: BATCH (60), \tLOSS = 1.521, \tACC:0.62\n",
            "EPOCH: 2: BATCH (65), \tLOSS = 1.561, \tACC:0.60\n",
            "EPOCH: 2: BATCH (70), \tLOSS = 1.496, \tACC:0.65\n",
            "EPOCH: 2: BATCH (75), \tLOSS = 1.496, \tACC:0.61\n",
            "EPOCH: 2: BATCH (80), \tLOSS = 1.484, \tACC:0.65\n",
            "EPOCH: 2: BATCH (85), \tLOSS = 1.416, \tACC:0.70\n",
            "EPOCH: 2: BATCH (90), \tLOSS = 1.514, \tACC:0.63\n",
            "EPOCH: 2: BATCH (95), \tLOSS = 1.425, \tACC:0.65\n",
            "EPOCH: 2: BATCH (100), \tLOSS = 1.466, \tACC:0.62\n",
            "EPOCH: 2: BATCH (105), \tLOSS = 1.451, \tACC:0.65\n",
            "EPOCH: 2: BATCH (110), \tLOSS = 1.473, \tACC:0.63\n",
            "EPOCH: 2: BATCH (115), \tLOSS = 1.459, \tACC:0.65\n",
            "TRAINING ON GPU\n",
            "EPOCH: 3: BATCH (0), \tLOSS = 1.506, \tACC:0.63\n",
            "EPOCH: 3: BATCH (5), \tLOSS = 1.489, \tACC:0.64\n",
            "EPOCH: 3: BATCH (10), \tLOSS = 1.489, \tACC:0.64\n",
            "EPOCH: 3: BATCH (15), \tLOSS = 1.439, \tACC:0.66\n",
            "EPOCH: 3: BATCH (20), \tLOSS = 1.471, \tACC:0.62\n",
            "EPOCH: 3: BATCH (25), \tLOSS = 1.459, \tACC:0.65\n",
            "EPOCH: 3: BATCH (30), \tLOSS = 1.471, \tACC:0.64\n",
            "EPOCH: 3: BATCH (35), \tLOSS = 1.476, \tACC:0.63\n",
            "EPOCH: 3: BATCH (40), \tLOSS = 1.441, \tACC:0.66\n",
            "EPOCH: 3: BATCH (45), \tLOSS = 1.462, \tACC:0.65\n",
            "EPOCH: 3: BATCH (50), \tLOSS = 1.448, \tACC:0.67\n",
            "EPOCH: 3: BATCH (55), \tLOSS = 1.452, \tACC:0.65\n",
            "EPOCH: 3: BATCH (60), \tLOSS = 1.432, \tACC:0.66\n",
            "EPOCH: 3: BATCH (65), \tLOSS = 1.466, \tACC:0.64\n",
            "EPOCH: 3: BATCH (70), \tLOSS = 1.462, \tACC:0.63\n",
            "EPOCH: 3: BATCH (75), \tLOSS = 1.449, \tACC:0.66\n",
            "EPOCH: 3: BATCH (80), \tLOSS = 1.463, \tACC:0.63\n",
            "EPOCH: 3: BATCH (85), \tLOSS = 1.477, \tACC:0.65\n",
            "EPOCH: 3: BATCH (90), \tLOSS = 1.498, \tACC:0.59\n",
            "EPOCH: 3: BATCH (95), \tLOSS = 1.493, \tACC:0.62\n",
            "EPOCH: 3: BATCH (100), \tLOSS = 1.418, \tACC:0.68\n",
            "EPOCH: 3: BATCH (105), \tLOSS = 1.456, \tACC:0.64\n",
            "EPOCH: 3: BATCH (110), \tLOSS = 1.462, \tACC:0.62\n",
            "EPOCH: 3: BATCH (115), \tLOSS = 1.454, \tACC:0.64\n",
            "TRAINING ON GPU\n",
            "EPOCH: 4: BATCH (0), \tLOSS = 1.467, \tACC:0.64\n",
            "EPOCH: 4: BATCH (5), \tLOSS = 1.446, \tACC:0.64\n",
            "EPOCH: 4: BATCH (10), \tLOSS = 1.486, \tACC:0.63\n",
            "EPOCH: 4: BATCH (15), \tLOSS = 1.456, \tACC:0.64\n",
            "EPOCH: 4: BATCH (20), \tLOSS = 1.406, \tACC:0.66\n",
            "EPOCH: 4: BATCH (25), \tLOSS = 1.454, \tACC:0.64\n",
            "EPOCH: 4: BATCH (30), \tLOSS = 1.415, \tACC:0.69\n",
            "EPOCH: 4: BATCH (35), \tLOSS = 1.421, \tACC:0.66\n",
            "EPOCH: 4: BATCH (40), \tLOSS = 1.391, \tACC:0.68\n",
            "EPOCH: 4: BATCH (45), \tLOSS = 1.473, \tACC:0.66\n",
            "EPOCH: 4: BATCH (50), \tLOSS = 1.421, \tACC:0.69\n",
            "EPOCH: 4: BATCH (55), \tLOSS = 1.454, \tACC:0.69\n",
            "EPOCH: 4: BATCH (60), \tLOSS = 1.422, \tACC:0.66\n",
            "EPOCH: 4: BATCH (65), \tLOSS = 1.436, \tACC:0.67\n",
            "EPOCH: 4: BATCH (70), \tLOSS = 1.405, \tACC:0.67\n",
            "EPOCH: 4: BATCH (75), \tLOSS = 1.411, \tACC:0.69\n",
            "EPOCH: 4: BATCH (80), \tLOSS = 1.429, \tACC:0.69\n",
            "EPOCH: 4: BATCH (85), \tLOSS = 1.411, \tACC:0.69\n",
            "EPOCH: 4: BATCH (90), \tLOSS = 1.433, \tACC:0.69\n",
            "EPOCH: 4: BATCH (95), \tLOSS = 1.464, \tACC:0.67\n",
            "EPOCH: 4: BATCH (100), \tLOSS = 1.437, \tACC:0.68\n",
            "EPOCH: 4: BATCH (105), \tLOSS = 1.432, \tACC:0.66\n",
            "EPOCH: 4: BATCH (110), \tLOSS = 1.474, \tACC:0.66\n",
            "EPOCH: 4: BATCH (115), \tLOSS = 1.428, \tACC:0.65\n",
            "TRAINING ON GPU\n",
            "EPOCH: 5: BATCH (0), \tLOSS = 1.425, \tACC:0.69\n",
            "EPOCH: 5: BATCH (5), \tLOSS = 1.423, \tACC:0.68\n",
            "EPOCH: 5: BATCH (10), \tLOSS = 1.435, \tACC:0.67\n",
            "EPOCH: 5: BATCH (15), \tLOSS = 1.392, \tACC:0.70\n",
            "EPOCH: 5: BATCH (20), \tLOSS = 1.448, \tACC:0.67\n",
            "EPOCH: 5: BATCH (25), \tLOSS = 1.443, \tACC:0.67\n",
            "EPOCH: 5: BATCH (30), \tLOSS = 1.435, \tACC:0.69\n",
            "EPOCH: 5: BATCH (35), \tLOSS = 1.426, \tACC:0.67\n",
            "EPOCH: 5: BATCH (40), \tLOSS = 1.413, \tACC:0.69\n",
            "EPOCH: 5: BATCH (45), \tLOSS = 1.392, \tACC:0.70\n",
            "EPOCH: 5: BATCH (50), \tLOSS = 1.412, \tACC:0.70\n",
            "EPOCH: 5: BATCH (55), \tLOSS = 1.426, \tACC:0.66\n",
            "EPOCH: 5: BATCH (60), \tLOSS = 1.429, \tACC:0.68\n",
            "EPOCH: 5: BATCH (65), \tLOSS = 1.404, \tACC:0.70\n",
            "EPOCH: 5: BATCH (70), \tLOSS = 1.433, \tACC:0.66\n",
            "EPOCH: 5: BATCH (75), \tLOSS = 1.450, \tACC:0.67\n",
            "EPOCH: 5: BATCH (80), \tLOSS = 1.422, \tACC:0.67\n",
            "EPOCH: 5: BATCH (85), \tLOSS = 1.405, \tACC:0.69\n",
            "EPOCH: 5: BATCH (90), \tLOSS = 1.417, \tACC:0.68\n",
            "EPOCH: 5: BATCH (95), \tLOSS = 1.399, \tACC:0.72\n",
            "EPOCH: 5: BATCH (100), \tLOSS = 1.422, \tACC:0.67\n",
            "EPOCH: 5: BATCH (105), \tLOSS = 1.441, \tACC:0.64\n",
            "EPOCH: 5: BATCH (110), \tLOSS = 1.414, \tACC:0.68\n",
            "EPOCH: 5: BATCH (115), \tLOSS = 1.456, \tACC:0.66\n",
            "TRAINING ON GPU\n",
            "EPOCH: 6: BATCH (0), \tLOSS = 1.419, \tACC:0.70\n",
            "EPOCH: 6: BATCH (5), \tLOSS = 1.397, \tACC:0.70\n",
            "EPOCH: 6: BATCH (10), \tLOSS = 1.426, \tACC:0.67\n",
            "EPOCH: 6: BATCH (15), \tLOSS = 1.413, \tACC:0.71\n",
            "EPOCH: 6: BATCH (20), \tLOSS = 1.428, \tACC:0.66\n",
            "EPOCH: 6: BATCH (25), \tLOSS = 1.386, \tACC:0.73\n",
            "EPOCH: 6: BATCH (30), \tLOSS = 1.383, \tACC:0.69\n",
            "EPOCH: 6: BATCH (35), \tLOSS = 1.424, \tACC:0.68\n",
            "EPOCH: 6: BATCH (40), \tLOSS = 1.432, \tACC:0.68\n",
            "EPOCH: 6: BATCH (45), \tLOSS = 1.406, \tACC:0.70\n",
            "EPOCH: 6: BATCH (50), \tLOSS = 1.402, \tACC:0.70\n",
            "EPOCH: 6: BATCH (55), \tLOSS = 1.418, \tACC:0.68\n",
            "EPOCH: 6: BATCH (60), \tLOSS = 1.433, \tACC:0.68\n",
            "EPOCH: 6: BATCH (65), \tLOSS = 1.464, \tACC:0.65\n",
            "EPOCH: 6: BATCH (70), \tLOSS = 1.385, \tACC:0.70\n",
            "EPOCH: 6: BATCH (75), \tLOSS = 1.367, \tACC:0.71\n",
            "EPOCH: 6: BATCH (80), \tLOSS = 1.390, \tACC:0.70\n",
            "EPOCH: 6: BATCH (85), \tLOSS = 1.420, \tACC:0.65\n",
            "EPOCH: 6: BATCH (90), \tLOSS = 1.396, \tACC:0.69\n",
            "EPOCH: 6: BATCH (95), \tLOSS = 1.400, \tACC:0.69\n",
            "EPOCH: 6: BATCH (100), \tLOSS = 1.390, \tACC:0.72\n",
            "EPOCH: 6: BATCH (105), \tLOSS = 1.385, \tACC:0.69\n",
            "EPOCH: 6: BATCH (110), \tLOSS = 1.393, \tACC:0.73\n",
            "EPOCH: 6: BATCH (115), \tLOSS = 1.404, \tACC:0.71\n",
            "TRAINING ON GPU\n",
            "EPOCH: 7: BATCH (0), \tLOSS = 1.432, \tACC:0.67\n",
            "EPOCH: 7: BATCH (5), \tLOSS = 1.388, \tACC:0.69\n",
            "EPOCH: 7: BATCH (10), \tLOSS = 1.431, \tACC:0.70\n",
            "EPOCH: 7: BATCH (15), \tLOSS = 1.397, \tACC:0.72\n",
            "EPOCH: 7: BATCH (20), \tLOSS = 1.421, \tACC:0.70\n",
            "EPOCH: 7: BATCH (25), \tLOSS = 1.429, \tACC:0.69\n",
            "EPOCH: 7: BATCH (30), \tLOSS = 1.389, \tACC:0.66\n",
            "EPOCH: 7: BATCH (35), \tLOSS = 1.427, \tACC:0.67\n",
            "EPOCH: 7: BATCH (40), \tLOSS = 1.394, \tACC:0.71\n",
            "EPOCH: 7: BATCH (45), \tLOSS = 1.416, \tACC:0.70\n",
            "EPOCH: 7: BATCH (50), \tLOSS = 1.370, \tACC:0.70\n",
            "EPOCH: 7: BATCH (55), \tLOSS = 1.386, \tACC:0.70\n",
            "EPOCH: 7: BATCH (60), \tLOSS = 1.427, \tACC:0.68\n",
            "EPOCH: 7: BATCH (65), \tLOSS = 1.410, \tACC:0.71\n",
            "EPOCH: 7: BATCH (70), \tLOSS = 1.423, \tACC:0.68\n",
            "EPOCH: 7: BATCH (75), \tLOSS = 1.367, \tACC:0.72\n",
            "EPOCH: 7: BATCH (80), \tLOSS = 1.376, \tACC:0.71\n",
            "EPOCH: 7: BATCH (85), \tLOSS = 1.390, \tACC:0.71\n",
            "EPOCH: 7: BATCH (90), \tLOSS = 1.414, \tACC:0.67\n",
            "EPOCH: 7: BATCH (95), \tLOSS = 1.401, \tACC:0.70\n",
            "EPOCH: 7: BATCH (100), \tLOSS = 1.385, \tACC:0.70\n",
            "EPOCH: 7: BATCH (105), \tLOSS = 1.405, \tACC:0.69\n",
            "EPOCH: 7: BATCH (110), \tLOSS = 1.390, \tACC:0.70\n",
            "EPOCH: 7: BATCH (115), \tLOSS = 1.430, \tACC:0.67\n",
            "TRAINING ON GPU\n",
            "EPOCH: 8: BATCH (0), \tLOSS = 1.356, \tACC:0.72\n",
            "EPOCH: 8: BATCH (5), \tLOSS = 1.431, \tACC:0.68\n",
            "EPOCH: 8: BATCH (10), \tLOSS = 1.352, \tACC:0.72\n",
            "EPOCH: 8: BATCH (15), \tLOSS = 1.435, \tACC:0.67\n",
            "EPOCH: 8: BATCH (20), \tLOSS = 1.419, \tACC:0.68\n",
            "EPOCH: 8: BATCH (25), \tLOSS = 1.380, \tACC:0.70\n",
            "EPOCH: 8: BATCH (30), \tLOSS = 1.422, \tACC:0.70\n",
            "EPOCH: 8: BATCH (35), \tLOSS = 1.394, \tACC:0.70\n",
            "EPOCH: 8: BATCH (40), \tLOSS = 1.404, \tACC:0.69\n",
            "EPOCH: 8: BATCH (45), \tLOSS = 1.366, \tACC:0.68\n",
            "EPOCH: 8: BATCH (50), \tLOSS = 1.398, \tACC:0.70\n",
            "EPOCH: 8: BATCH (55), \tLOSS = 1.372, \tACC:0.70\n",
            "EPOCH: 8: BATCH (60), \tLOSS = 1.386, \tACC:0.69\n",
            "EPOCH: 8: BATCH (65), \tLOSS = 1.370, \tACC:0.73\n",
            "EPOCH: 8: BATCH (70), \tLOSS = 1.413, \tACC:0.65\n",
            "EPOCH: 8: BATCH (75), \tLOSS = 1.392, \tACC:0.69\n",
            "EPOCH: 8: BATCH (80), \tLOSS = 1.420, \tACC:0.68\n",
            "EPOCH: 8: BATCH (85), \tLOSS = 1.404, \tACC:0.70\n",
            "EPOCH: 8: BATCH (90), \tLOSS = 1.367, \tACC:0.70\n",
            "EPOCH: 8: BATCH (95), \tLOSS = 1.420, \tACC:0.68\n",
            "EPOCH: 8: BATCH (100), \tLOSS = 1.386, \tACC:0.70\n",
            "EPOCH: 8: BATCH (105), \tLOSS = 1.378, \tACC:0.73\n",
            "EPOCH: 8: BATCH (110), \tLOSS = 1.418, \tACC:0.68\n",
            "EPOCH: 8: BATCH (115), \tLOSS = 1.418, \tACC:0.66\n",
            "TRAINING ON GPU\n",
            "EPOCH: 9: BATCH (0), \tLOSS = 1.380, \tACC:0.72\n",
            "EPOCH: 9: BATCH (5), \tLOSS = 1.388, \tACC:0.69\n",
            "EPOCH: 9: BATCH (10), \tLOSS = 1.374, \tACC:0.70\n",
            "EPOCH: 9: BATCH (15), \tLOSS = 1.377, \tACC:0.71\n",
            "EPOCH: 9: BATCH (20), \tLOSS = 1.372, \tACC:0.70\n",
            "EPOCH: 9: BATCH (25), \tLOSS = 1.420, \tACC:0.69\n",
            "EPOCH: 9: BATCH (30), \tLOSS = 1.352, \tACC:0.71\n",
            "EPOCH: 9: BATCH (35), \tLOSS = 1.377, \tACC:0.71\n",
            "EPOCH: 9: BATCH (40), \tLOSS = 1.390, \tACC:0.68\n",
            "EPOCH: 9: BATCH (45), \tLOSS = 1.374, \tACC:0.71\n",
            "EPOCH: 9: BATCH (50), \tLOSS = 1.354, \tACC:0.70\n",
            "EPOCH: 9: BATCH (55), \tLOSS = 1.433, \tACC:0.68\n",
            "EPOCH: 9: BATCH (60), \tLOSS = 1.386, \tACC:0.70\n",
            "EPOCH: 9: BATCH (65), \tLOSS = 1.409, \tACC:0.68\n",
            "EPOCH: 9: BATCH (70), \tLOSS = 1.413, \tACC:0.71\n",
            "EPOCH: 9: BATCH (75), \tLOSS = 1.382, \tACC:0.72\n",
            "EPOCH: 9: BATCH (80), \tLOSS = 1.407, \tACC:0.69\n",
            "EPOCH: 9: BATCH (85), \tLOSS = 1.383, \tACC:0.69\n",
            "EPOCH: 9: BATCH (90), \tLOSS = 1.384, \tACC:0.70\n",
            "EPOCH: 9: BATCH (95), \tLOSS = 1.411, \tACC:0.68\n",
            "EPOCH: 9: BATCH (100), \tLOSS = 1.393, \tACC:0.69\n",
            "EPOCH: 9: BATCH (105), \tLOSS = 1.390, \tACC:0.68\n",
            "EPOCH: 9: BATCH (110), \tLOSS = 1.355, \tACC:0.73\n",
            "EPOCH: 9: BATCH (115), \tLOSS = 1.358, \tACC:0.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTZ-0_aJQTCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "50edff45-b042-49fb-a4ce-6e0fd2857ec0"
      },
      "source": [
        "torch.save(network, \"saved_model.pth\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Network. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SELECT. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type BinLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type AND. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type XOR. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type OR. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type NotLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIL0TEMKb4z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "92bb10a1-694c-4314-dc83-78d56fdde1be"
      },
      "source": [
        "test(network)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TESTING ON GPU\n",
            "BATCH (0), \tLOSS = 1.371, \tACC:0.71\n",
            "BATCH (5), \tLOSS = 1.368, \tACC:0.73\n",
            "BATCH (10), \tLOSS = 1.327, \tACC:0.76\n",
            "BATCH (15), \tLOSS = 1.359, \tACC:0.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFJM-8UmX7Y0",
        "colab_type": "text"
      },
      "source": [
        "# Check the confidence of each choice so that we can freeze and convert them into gates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e99A9C-HO2Oc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "35ff9c23-9011-467f-bb4a-0220f36bc8ee"
      },
      "source": [
        "for name, param in network.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        if \"select\" in name:\n",
        "            print(\"SELECT\")\n",
        "            softmax = nn.Softmax(dim = 0)\n",
        "            weights = param.data / 0.01\n",
        "            outs = torch.transpose(softmax(weights), 0, 1)\n",
        "            outs, _ = torch.max(outs, dim = -1)\n",
        "            print(torch.mean(outs))\n",
        "            \n",
        "        elif \"bin\" in name:\n",
        "            print(\"BIN\")\n",
        "            softmax = nn.Softmax(dim = -1)\n",
        "            weights = param.data / 0.01\n",
        "            outs, _ = torch.max(softmax(weights), dim = -1)\n",
        "            print(torch.mean(outs))\n",
        "        else:\n",
        "            print(\"UNKNOWN\")\n",
        "        #print(name, param.data)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SELECT\n",
            "tensor(0.9382, device='cuda:0')\n",
            "BIN\n",
            "tensor(0.9980, device='cuda:0')\n",
            "SELECT\n",
            "tensor(0.9382, device='cuda:0')\n",
            "UNKNOWN\n",
            "BIN\n",
            "tensor(0.9984, device='cuda:0')\n",
            "SELECT\n",
            "tensor(0.9388, device='cuda:0')\n",
            "UNKNOWN\n",
            "BIN\n",
            "tensor(0.9980, device='cuda:0')\n",
            "SELECT\n",
            "tensor(0.9251, device='cuda:0')\n",
            "UNKNOWN\n",
            "BIN\n",
            "tensor(0.9970, device='cuda:0')\n",
            "SELECT\n",
            "tensor(0.9166, device='cuda:0')\n",
            "UNKNOWN\n",
            "BIN\n",
            "tensor(0.9934, device='cuda:0')\n",
            "SELECT\n",
            "tensor(0.9854, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp9mm14VO9qt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}